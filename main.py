import os
import time
import logging
import requests
from openai import OpenAI
import json
from urllib.parse import urljoin, urlparse
from flask import Flask, request, jsonify
from bs4 import BeautifulSoup
from datetime import datetime

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)

# ‚Äî‚Äî Config ‚Äî‚Äî #
ROOT_URL             = "https://docs.cloud.mn/"
DELAY_SEC            = 0.5
ALLOWED_NETLOC       = urlparse(ROOT_URL).netloc
MAX_CRAWL_PAGES      = 50
CHATWOOT_API_KEY     = os.getenv("CHATWOOT_API_KEY")
ACCOUNT_ID           = os.getenv("ACCOUNT_ID")
CHATWOOT_BASE_URL    = os.getenv("CHATWOOT_BASE_URL", "https://app.chatwoot.com")
OPENAI_API_KEY       = os.getenv("OPENAI_API_KEY")

# Initialize OpenAI client
client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

# ‚Äî‚Äî Memory Storage ‚Äî‚Äî #
conversation_memory = {}
crawled_data = []

# ‚Äî‚Äî Content Extraction ‚Äî‚Äî #
def extract_content(soup: BeautifulSoup, base_url: str):
    main = soup.find("main") or soup
    texts = []
    images = []

    for tag in main.find_all(["h1", "h2", "h3", "h4", "p", "li", "code"]):
        text = tag.get_text(strip=True)
        if text:
            texts.append(text)

    for img in main.find_all("img"):
        src = img.get("src")
        alt = img.get("alt", "").strip()
        if src:
            full_img_url = urljoin(base_url, src)
            entry = f"[Image] {alt} ‚Äî {full_img_url}" if alt else f"[Image] {full_img_url}"
            texts.append(entry)
            images.append({"url": full_img_url, "alt": alt})

    return "\n\n".join(texts), images

def is_internal_link(href: str) -> bool:
    if not href:
        return False
    parsed = urlparse(href)
    return not parsed.netloc or parsed.netloc == ALLOWED_NETLOC

def normalize_url(base: str, link: str) -> str:
    return urljoin(base, link.split("#")[0])


# ‚Äî‚Äî Crawl & Scrape ‚Äî‚Äî #
def crawl_and_scrape(start_url: str):
    visited = set()
    to_visit = {start_url}
    results = []

    while to_visit and len(visited) < MAX_CRAWL_PAGES:
        url = to_visit.pop()
        if url in visited:
            continue
        visited.add(url)

        try:
            logging.info(f"[Crawling] {url}")
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
        except Exception as e:
            logging.warning(f"Failed to fetch {url}: {e}")
            continue

        soup = BeautifulSoup(resp.text, "html.parser")
        title = soup.title.string.strip() if soup.title else url
        body, images = extract_content(soup, url)

        results.append({
            "url": url,
            "title": title,
            "body": body,
            "images": images
        })

        for a in soup.find_all("a", href=True):
            href = a["href"]
            if is_internal_link(href):
                full = normalize_url(url, href)
                if full.startswith(ROOT_URL) and full not in visited:
                    to_visit.add(full)

        time.sleep(DELAY_SEC)

    return results

def scrape_single(url: str):
    resp = requests.get(url, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    title = soup.title.string.strip() if soup.title else url
    body, images = extract_content(soup, url)
    return {"url": url, "title": title, "body": body, "images": images}


# ‚Äî‚Äî AI Assistant Functions ‚Äî‚Äî #
def get_ai_response(user_message: str, conversation_id: int, context_data: list = None):
    """Enhanced AI response with context awareness"""
    
    if not client:
        return "üîë OpenAI API —Ç“Ø–ª—Ö“Ø“Ø—Ä —Ç–æ—Ö–∏—Ä—É—É–ª–∞–≥–¥–∞–∞–≥“Ø–π –±–∞–π–Ω–∞. –ê–¥–º–∏–Ω—Ç–∞–π —Ö–æ–ª–±–æ–≥–¥–æ–Ω–æ —É—É."
    
    # Get conversation history
    history = conversation_memory.get(conversation_id, [])
    
    # Build context from crawled data if available
    context = ""
    if context_data and crawled_data:
        relevant_pages = []
        for page in crawled_data[:3]:  # Use first 3 pages as context
            relevant_pages.append(f"–•—É—É–¥–∞—Å: {page['title']}\n–ê–≥—É—É–ª–≥–∞: {page['body'][:300]}...")
        context = "\n\n".join(relevant_pages)
    
    # Build conversation context
    messages = [
        {
            "role": "system", 
            "content": f"""–¢–∞ Cloud.mn-–∏–π–Ω –±–∞—Ä–∏–º—Ç –±–∏—á–≥–∏–π–Ω —Ç–∞–ª–∞–∞—Ä –∞—Å—É—É–ª—Ç–∞–¥ —Ö–∞—Ä–∏—É–ª–¥–∞–≥ –ú–æ–Ω–≥–æ–ª AI —Ç—É—Å–ª–∞—Ö —é–º. 
            –•—ç—Ä—ç–≥–ª—ç–≥—á—Ç—ç–π –º–æ–Ω–≥–æ–ª —Ö—ç–ª—ç—ç—Ä —è—Ä–∏–ª—Ü–∞–∞—Ä–∞–π. –•–∞—Ä–∏—É–ª—Ç–∞–∞ —Ç–æ–≤—á –±”©–≥”©”©–¥ –æ–π–ª–≥–æ–º–∂—Ç–æ–π –±–∞–π–ª–≥–∞–∞—Ä–∞–π.
            
            –ë–æ–ª–æ–º–∂–∏—Ç –∫–æ–º–∞–Ω–¥—É—É–¥:
            - crawl: –ë“Ø—Ö —Å–∞–π—Ç—ã–≥ —à“Ø“Ø—Ä–¥—ç—Ö
            - scrape <URL>: –¢–æ–¥–æ—Ä—Ö–æ–π —Ö—É—É–¥—Å—ã–≥ —à“Ø“Ø—Ä–¥—ç—Ö  
            - help: –¢—É—Å–ª–∞–º–∂ —Ö–∞—Ä—É—É–ª–∞—Ö
            - search <–∞—Å—É—É–ª—Ç>: –ú—ç–¥—ç—ç–ª—ç–ª —Ö–∞–π—Ö
            
            {f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –º—ç–¥—ç—ç–ª—ç–ª:\\n{context}" if context else ""}
            """
        }
    ]
    
    # Add conversation history
    for msg in history[-4:]:  # Last 4 messages
        messages.append(msg)
    
    # Add current message
    messages.append({"role": "user", "content": user_message})
    
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            max_tokens=300,
            temperature=0.7
        )
        
        ai_response = response.choices[0].message.content
        
        # Store in memory
        if conversation_id not in conversation_memory:
            conversation_memory[conversation_id] = []
        
        conversation_memory[conversation_id].append({"role": "user", "content": user_message})
        conversation_memory[conversation_id].append({"role": "assistant", "content": ai_response})
        
        # Keep only last 8 messages
        if len(conversation_memory[conversation_id]) > 8:
            conversation_memory[conversation_id] = conversation_memory[conversation_id][-8:]
            
        return ai_response
        
    except Exception as e:
        logging.error(f"OpenAI API –∞–ª–¥–∞–∞: {e}")
        return f"üîß AI-—Ç–∞–π —Ö–æ–ª–±–æ–≥–¥–æ—Ö–æ–¥ —Å–∞–∞–¥ –≥–∞—Ä–ª–∞–∞. –¢–∞ –¥–∞—Ä–∞–∞—Ö –∞—Ä–≥—É—É–¥–∞–∞—Ä —Ç—É—Å–ª–∞–º–∂ –∞–≤—á –±–æ–ª–Ω–æ:\n‚Ä¢ 'help' –∫–æ–º–∞–Ω–¥—ã–≥ –∞—à–∏–≥–ª–∞–Ω–∞ —É—É\n‚Ä¢ 'crawl' —ç—Å–≤—ç–ª 'search' –∫–æ–º–∞–Ω–¥—É—É–¥—ã–≥ —Ç—É—Ä—à–∏–Ω–∞ —É—É\n\n–ê–ª–¥–∞–∞–Ω—ã –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π: {str(e)[:100]}"

def search_in_crawled_data(query: str, max_results: int = 3):
    """Search through crawled data"""
    if not crawled_data:
        return []
    
    query_lower = query.lower()
    results = []
    
    for page in crawled_data:
        title_match = query_lower in page['title'].lower()
        body_match = query_lower in page['body'].lower()
        
        if title_match or body_match:
            results.append({
                'title': page['title'],
                'url': page['url'],
                'snippet': page['body'][:200] + "..." if len(page['body']) > 200 else page['body']
            })
            
        if len(results) >= max_results:
            break
            
    return results


# ‚Äî‚Äî Enhanced Chatwoot Integration ‚Äî‚Äî #
def send_to_chatwoot(conv_id: int, content: str, message_type: str = "outgoing"):
    """Enhanced chatwoot message sending with better error handling"""
    api_url = (
        f"{CHATWOOT_BASE_URL}/api/v1/accounts/{ACCOUNT_ID}"
        f"/conversations/{conv_id}/messages"
    )
    headers = {
        "api_access_token": CHATWOOT_API_KEY,
        "Content-Type": "application/json"
    }
    payload = {
        "content": content, 
        "message_type": message_type,
        "private": False
    }
    
    try:
        resp = requests.post(api_url, json=payload, headers=headers, timeout=10)
        resp.raise_for_status()
        logging.info(f"Message sent to conversation {conv_id}")
        return True
    except Exception as e:
        logging.error(f"Failed to send message to chatwoot: {e}")
        return False

def get_conversation_info(conv_id: int):
    """Get conversation details from Chatwoot"""
    api_url = f"{CHATWOOT_BASE_URL}/api/v1/accounts/{ACCOUNT_ID}/conversations/{conv_id}"
    headers = {"api_access_token": CHATWOOT_API_KEY}
    
    try:
        resp = requests.get(api_url, headers=headers, timeout=10)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        logging.error(f"Failed to get conversation info: {e}")
        return None

def mark_conversation_resolved(conv_id: int):
    """Mark conversation as resolved"""
    api_url = f"{CHATWOOT_BASE_URL}/api/v1/accounts/{ACCOUNT_ID}/conversations/{conv_id}/toggle_status"
    headers = {"api_access_token": CHATWOOT_API_KEY}
    payload = {"status": "resolved"}
    
    try:
        resp = requests.post(api_url, json=payload, headers=headers, timeout=10)
        resp.raise_for_status()
        return True
    except Exception as e:
        logging.error(f"Failed to mark conversation as resolved: {e}")
        return False


# ‚Äî‚Äî API Endpoints ‚Äî‚Äî #
@app.route("/api/scrape", methods=["POST"])
def api_scrape():
    data = request.get_json(force=True)
    url = data.get("url")
    if not url:
        return jsonify({"error": "Missing 'url' in JSON body"}), 400
    try:
        page = scrape_single(url)
        return jsonify(page)
    except Exception as e:
        return jsonify({"error": f"Fetch/Scrape failed: {e}"}), 502

@app.route("/api/crawl", methods=["POST"])
def api_crawl():
    pages = crawl_and_scrape(ROOT_URL)
    return jsonify(pages)


# ‚Äî‚Äî Enhanced Chatwoot Webhook ‚Äî‚Äî #
@app.route("/webhook/chatwoot", methods=["POST"])
def chatwoot_webhook():
    """Enhanced webhook with better AI integration"""
    data = request.json or {}
    
    # Only process incoming messages
    if data.get("message_type") != "incoming":
        return jsonify({}), 200

    conv_id = data["conversation"]["id"]
    text = data.get("content", "").strip()
    contact = data.get("conversation", {}).get("contact", {})
    contact_name = contact.get("name", "–•—ç—Ä—ç–≥–ª—ç–≥—á")
    
    logging.info(f"Received message from {contact_name} in conversation {conv_id}: {text}")
    
    # Handle different commands
    if text.lower() == "crawl":
        send_to_chatwoot(conv_id, f"üîÑ –°–∞–π–Ω –±–∞–π–Ω–∞ —É—É {contact_name}! –°–∞–π—Ç—ã–≥ —à“Ø“Ø—Ä–¥—ç–∂ –±–∞–π–Ω–∞, —Ç“Ø—Ä —Ö“Ø–ª—ç—ç–Ω—ç “Ø“Ø...")
        
        global crawled_data
        crawled_data = crawl_and_scrape(ROOT_URL)
        
        if not crawled_data:
            send_to_chatwoot(conv_id, "‚ùå –®“Ø“Ø—Ä–¥—ç—Ö —è–≤—Ü–∞–¥ –∞–ª–¥–∞–∞ –≥–∞—Ä–ª–∞–∞. –î–∞—Ö–∏–Ω –æ—Ä–æ–ª–¥–æ–Ω–æ —É—É.")
        else:
            lines = [f"üìÑ {p['title']} ‚Äî {p['url']}" for p in crawled_data[:3]]
            send_to_chatwoot(conv_id,
                f"‚úÖ {len(crawled_data)} —Ö—É—É–¥–∞—Å –∞–º–∂–∏–ª—Ç—Ç–∞–π —à“Ø“Ø—Ä–¥–ª—ç—ç!\n\n"
                f"–≠—Ö–Ω–∏–π 3 —Ö—É—É–¥–∞—Å:\n" + "\n".join(lines) + 
                f"\n\n–û–¥–æ–æ 'search <–∞—Å—É—É–ª—Ç>' –∫–æ–º–∞–Ω–¥–∞–∞—Ä —Ö–∞–π–ª—Ç —Ö–∏–π–∂ –±–æ–ª–Ω–æ!"
            )

    elif text.lower().startswith("scrape"):
        parts = text.split(maxsplit=1)
        if len(parts) < 2:
            send_to_chatwoot(conv_id, "‚ö†Ô∏è –ó”©–≤ —Ö—ç–ª–±—ç—Ä: `scrape <–±“Ø—Ä—ç–Ω-URL>`")
        else:
            url = parts[1].strip()
            send_to_chatwoot(conv_id, f"üîÑ {url} —Ö–∞—è–≥—ã–≥ —à“Ø“Ø—Ä–¥—ç–∂ –±–∞–π–Ω–∞...")
            
            try:
                page = scrape_single(url)
                summary = get_ai_response(f"–≠–Ω—ç –∞–≥—É—É–ª–≥—ã–≥ —Ç–æ–≤—á–ª–æ–Ω —Ö—ç–ª—ç—ç—Ä—ç–π: {page['body'][:1500]}", conv_id)
                
                send_to_chatwoot(conv_id,
                    f"üìÑ **{page['title']}**\n\n"
                    f"üìù **–¢–æ–≤—á–∏–ª—Å–æ–Ω –∞–≥—É—É–ª–≥–∞:**\n{summary}\n\n"
                    f"üîó {url}"
                )
            except Exception as e:
                send_to_chatwoot(conv_id, f"‚ùå {url} —Ö–∞—è–≥—ã–≥ —à“Ø“Ø—Ä–¥—ç—Ö—ç–¥ –∞–ª–¥–∞–∞ –≥–∞—Ä–ª–∞–∞: {e}")

    elif text.lower().startswith("search"):
        parts = text.split(maxsplit=1)
        if len(parts) < 2:
            send_to_chatwoot(conv_id, "‚ö†Ô∏è –ó”©–≤ —Ö—ç–ª–±—ç—Ä: `search <—Ö–∞–π—Ö “Ø–≥>`")
        else:
            query = parts[1].strip()
            
            if not crawled_data:
                send_to_chatwoot(conv_id, 
                    "üìö –≠—Ö–ª—ç—ç–¥ 'crawl' –∫–æ–º–∞–Ω–¥—ã–≥ –∞—à–∏–≥–ª–∞–Ω —Å–∞–π—Ç—ã–≥ —à“Ø“Ø—Ä–¥“Ø“Ø–ª–Ω—ç “Ø“Ø."
                )
            else:
                send_to_chatwoot(conv_id, f"üîç '{query}' —Ö–∞–π–∂ –±–∞–π–Ω–∞...")
                
                results = search_in_crawled_data(query)
                if results:
                    response = f"üîç '{query}' —Ö–∞–π–ª—Ç—ã–Ω “Ø—Ä –¥“Ø–Ω:\n\n"
                    for i, result in enumerate(results, 1):
                        response += f"{i}. **{result['title']}**\n"
                        response += f"   {result['snippet']}\n"
                        response += f"   üîó {result['url']}\n\n"
                    
                    send_to_chatwoot(conv_id, response)
                else:
                    send_to_chatwoot(conv_id, f"‚ùå '{query}' —Ö–∞–π–ª—Ç–∞–∞—Ä –∏–ª—ç—Ä—Ü –æ–ª–¥—Å–æ–Ω–≥“Ø–π.")

    elif text.lower() in ["help", "—Ç—É—Å–ª–∞–º–∂"]:
        help_text = f"""
üëã –°–∞–π–Ω –±–∞–π–Ω–∞ —É—É {contact_name}! –ë–∏ Cloud.mn-–∏–π–Ω AI —Ç—É—Å–ª–∞—Ö —é–º.

ü§ñ **–ë–æ–ª–æ–º–∂–∏—Ç –∫–æ–º–∞–Ω–¥—É—É–¥:**
‚Ä¢ `crawl` - –ë“Ø—Ö —Å–∞–π—Ç—ã–≥ —à“Ø“Ø—Ä–¥—ç—Ö
‚Ä¢ `scrape <URL>` - –¢–æ–¥–æ—Ä—Ö–æ–π —Ö—É—É–¥–∞—Å —à“Ø“Ø—Ä–¥—ç—Ö
‚Ä¢ `search <–∞—Å—É—É–ª—Ç>` - –ú—ç–¥—ç—ç–ª—ç–ª —Ö–∞–π—Ö
‚Ä¢ `help` - –≠–Ω—ç —Ç—É—Å–ª–∞–º–∂–∏–π–≥ —Ö–∞—Ä—É—É–ª–∞—Ö

üí¨ **–ß”©–ª”©”©—Ç —è—Ä–∏–ª—Ü–ª–∞–≥–∞:**
–¢–∞ –º”©–Ω –Ω–∞–¥–∞–¥ –∞—Å—É—É–ª—Ç –∞—Å—É—É–∂, —è—Ä–∏–ª—Ü–∞–∂ –±–æ–ª–Ω–æ. –ë–∏ –º–æ–Ω–≥–æ–ª —Ö—ç–ª—ç—ç—Ä —Ö–∞—Ä–∏—É–ª–Ω–∞.

‚è∞ “Æ—Ä–≥—ç–ª–∂ —Ç—É—Å–ª–∞–º–∂–∏–¥ –±—ç–ª—ç–Ω –±–∞–π–Ω–∞!
        """
        send_to_chatwoot(conv_id, help_text)

    elif text.lower() in ["–±–∞—è—Ä—Ç–∞–π", "goodbye", "–±–∞–∞–π"]:
        send_to_chatwoot(conv_id, f"üëã –ë–∞—è—Ä—Ç–∞–π {contact_name}! –î–∞—Ä–∞–∞ —É—É–ª–∑–∞—Ü–≥–∞–∞—è!")
        mark_conversation_resolved(conv_id)

    else:
        # General AI conversation
        send_to_chatwoot(conv_id, "ü§î –ë–æ–ª–æ–≤—Å—Ä—É—É–ª–∂ –±–∞–π–Ω–∞...")
        ai_response = get_ai_response(text, conv_id, crawled_data)
        send_to_chatwoot(conv_id, ai_response)

    return jsonify({"status": "success"}), 200


# ‚Äî‚Äî Additional API Endpoints ‚Äî‚Äî #
@app.route("/api/conversation/<int:conv_id>/memory", methods=["GET"])
def get_conversation_memory(conv_id):
    """Get conversation memory for debugging"""
    memory = conversation_memory.get(conv_id, [])
    return jsonify({"conversation_id": conv_id, "memory": memory})

@app.route("/api/conversation/<int:conv_id>/clear", methods=["POST"])
def clear_conversation_memory(conv_id):
    """Clear conversation memory"""
    if conv_id in conversation_memory:
        del conversation_memory[conv_id]
    return jsonify({"status": "cleared", "conversation_id": conv_id})

@app.route("/api/crawled-data", methods=["GET"])
def get_crawled_data():
    """Get current crawled data"""
    return jsonify({"pages": len(crawled_data), "data": crawled_data[:10]})  # First 10 pages

@app.route("/health", methods=["GET"])
def health_check():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "crawled_pages": len(crawled_data),
        "active_conversations": len(conversation_memory)
    })


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)
