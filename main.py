import os
import time
import logging
import requests
from openai import OpenAI
import json
from urllib.parse import urljoin, urlparse
from flask import Flask, request, jsonify
from bs4 import BeautifulSoup
from datetime import datetime

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)

# ‚Äî‚Äî Config ‚Äî‚Äî #
ROOT_URL             = os.getenv("ROOT_URL", "https://docs.cloud.mn/")
DELAY_SEC            = float(os.getenv("DELAY_SEC", "0.5"))
ALLOWED_NETLOC       = urlparse(ROOT_URL).netloc
MAX_CRAWL_PAGES      = int(os.getenv("MAX_CRAWL_PAGES", "50"))
CHATWOOT_API_KEY     = os.getenv("CHATWOOT_API_KEY")
ACCOUNT_ID           = os.getenv("ACCOUNT_ID")
CHATWOOT_BASE_URL    = os.getenv("CHATWOOT_BASE_URL", "https://app.chatwoot.com")
OPENAI_API_KEY       = os.getenv("OPENAI_API_KEY")
AUTO_CRAWL_ON_START  = os.getenv("AUTO_CRAWL_ON_START", "true").lower() == "true"
TEAMS_WEBHOOK_URL    = os.getenv("TEAMS_WEBHOOK_URL")  # Microsoft Teams webhook URL

# Initialize OpenAI client
client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

# ‚Äî‚Äî Memory Storage ‚Äî‚Äî #
conversation_memory = {}
crawled_data = []
crawl_status = {"status": "not_started", "message": "Crawling has not started yet"}

# ‚Äî‚Äî Crawl & Scrape ‚Äî‚Äî #
def crawl_and_scrape(start_url: str):
    visited = set()
    to_visit = {start_url}
    results = []

    while to_visit and len(visited) < MAX_CRAWL_PAGES:
        url = to_visit.pop()
        if url in visited:
            continue
        visited.add(url)

        try:
            logging.info(f"[Crawling] {url}")
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
        except Exception as e:
            logging.warning(f"Failed to fetch {url}: {e}")
            continue

        soup = BeautifulSoup(resp.text, "html.parser")
        title = soup.title.string.strip() if soup.title else url
        body, images = extract_content(soup, url)

        results.append({
            "url": url,
            "title": title,
            "body": body,
            "images": images
        })

        for a in soup.find_all("a", href=True):
            href = a["href"]
            if is_internal_link(href):
                full = normalize_url(url, href)
                if full.startswith(ROOT_URL) and full not in visited:
                    to_visit.add(full)

        time.sleep(DELAY_SEC)

    return results

# ‚Äî‚Äî Startup Functions ‚Äî‚Äî #
def auto_crawl_on_startup():
    """Automatically crawl the site on startup"""
    global crawled_data, crawl_status
    
    if not AUTO_CRAWL_ON_START:
        crawl_status = {"status": "disabled", "message": "Auto-crawl is disabled"}
        logging.info("Auto-crawl is disabled")
        return
    
    try:
        logging.info(f"üöÄ Starting automatic crawl of {ROOT_URL}")
        crawl_status = {"status": "running", "message": f"Crawling {ROOT_URL}..."}
        
        crawled_data = crawl_and_scrape(ROOT_URL)
        
        if crawled_data:
            crawl_status = {
                "status": "completed", 
                "message": f"Successfully crawled {len(crawled_data)} pages",
                "pages_count": len(crawled_data),
                "timestamp": datetime.now().isoformat()
            }
            logging.info(f"‚úÖ Auto-crawl completed: {len(crawled_data)} pages")
        else:
            crawl_status = {"status": "failed", "message": "No pages were crawled"}
            logging.warning("‚ùå Auto-crawl failed: No pages found")
            
    except Exception as e:
        crawl_status = {"status": "error", "message": f"Crawl error: {str(e)}"}
        logging.error(f"‚ùå Auto-crawl error: {e}")

# Start auto-crawl in background when app starts
import threading
if AUTO_CRAWL_ON_START:
    threading.Thread(target=auto_crawl_on_startup, daemon=True).start()

# ‚Äî‚Äî Content Extraction ‚Äî‚Äî #
def extract_content(soup: BeautifulSoup, base_url: str):
    main = soup.find("main") or soup
    texts = []
    images = []

    for tag in main.find_all(["h1", "h2", "h3", "h4", "p", "li", "code"]):
        text = tag.get_text(strip=True)
        if text:
            texts.append(text)

    for img in main.find_all("img"):
        src = img.get("src")
        alt = img.get("alt", "").strip()
        if src:
            full_img_url = urljoin(base_url, src)
            entry = f"[Image] {alt} ‚Äî {full_img_url}" if alt else f"[Image] {full_img_url}"
            texts.append(entry)
            images.append({"url": full_img_url, "alt": alt})

    return "\n\n".join(texts), images

def is_internal_link(href: str) -> bool:
    if not href:
        return False
    parsed = urlparse(href)
    return not parsed.netloc or parsed.netloc == ALLOWED_NETLOC

def normalize_url(base: str, link: str) -> str:
    return urljoin(base, link.split("#")[0])

def scrape_single(url: str):
    resp = requests.get(url, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    title = soup.title.string.strip() if soup.title else url
    body, images = extract_content(soup, url)
    return {"url": url, "title": title, "body": body, "images": images}


# ‚Äî‚Äî AI Assistant Functions ‚Äî‚Äî #
def get_ai_response(user_message: str, conversation_id: int, context_data: list = None):
    """Enhanced AI response with better context awareness and image support"""
    
    if not client:
        return "üîë OpenAI API —Ç“Ø–ª—Ö“Ø“Ø—Ä —Ç–æ—Ö–∏—Ä—É—É–ª–∞–≥–¥–∞–∞–≥“Ø–π –±–∞–π–Ω–∞. –ê–¥–º–∏–Ω—Ç–∞–π —Ö–æ–ª–±–æ–≥–¥–æ–Ω–æ —É—É."
    
    # Get conversation history
    history = conversation_memory.get(conversation_id, [])
    
    # Build context from crawled data if available
    context = ""
    if context_data and crawled_data:
        # Search for relevant content
        search_results = search_in_crawled_data(user_message, max_results=3)
        if search_results:
            relevant_pages = []
            for result in search_results:
                # Find the page in crawled_data to get images
                page = next((p for p in crawled_data if p['url'] == result['url']), None)
                if page and page.get('images'):
                    image_info = "\n–ó—É—Ä–≥—É—É–¥:\n" + "\n".join([
                        f"- {img['alt']}: {img['url']}" if img['alt'] else f"- {img['url']}"
                        for img in page['images']
                    ])
                else:
                    image_info = ""
                
                relevant_pages.append(
                    f"–•—É—É–¥–∞—Å: {result['title']}\n"
                    f"URL: {result['url']}\n"
                    f"–•–æ–ª–±–æ–≥–¥–æ—Ö –∞–≥—É—É–ª–≥–∞: {result['snippet']}\n"
                    f"{image_info}\n"
                )
            context = "\n\n".join(relevant_pages)
    
    # Build system message with context
    system_content = """–¢–∞ Cloud.mn-–∏–π–Ω –±–∞—Ä–∏–º—Ç –±–∏—á–≥–∏–π–Ω —Ç–∞–ª–∞–∞—Ä –∞—Å—É—É–ª—Ç–∞–¥ —Ö–∞—Ä–∏—É–ª–¥–∞–≥ –ú–æ–Ω–≥–æ–ª AI —Ç—É—Å–ª–∞—Ö —é–º. 
    –•—ç—Ä—ç–≥–ª—ç–≥—á—Ç—ç–π –º–æ–Ω–≥–æ–ª —Ö—ç–ª—ç—ç—Ä —è—Ä–∏–ª—Ü–∞–∞—Ä–∞–π. –•–∞—Ä–∏—É–ª—Ç–∞–∞ —Ç–æ–≤—á –±”©–≥”©”©–¥ –æ–π–ª–≥–æ–º–∂—Ç–æ–π –±–∞–π–ª–≥–∞–∞—Ä–∞–π.
    
    –•–∞—Ä–∏—É–ª–∞—Ö–¥–∞–∞ –¥–∞—Ä–∞–∞—Ö –∑“Ø–π–ª—Å–∏–π–≥ –∞–Ω—Ö–∞–∞—Ä–Ω–∞ —É—É:
    1. –•–∞—Ä–∏—É–ª—Ç–∞–∞ —Ö–æ–ª–±–æ–≥–¥–æ—Ö –±–∞—Ä–∏–º—Ç –±–∏—á–≥–∏–π–Ω –ª–∏–Ω–∫—ç—ç—Ä –¥—ç–º–∂“Ø“Ø–ª—ç—ç—Ä—ç–π
    2. –•—ç—Ä—ç–≤ –æ–π–ª–≥–æ–º–∂–≥“Ø–π –±–æ–ª —Ç–æ–¥–æ—Ä—Ö–æ–π –∞—Å—É—É–Ω–∞ —É—É
    3. –•–∞—Ä–∏—É–ª—Ç–∞–∞ –±“Ø—Ç—ç—Ü—Ç—ç–π, —Ü—ç–≥—Ü—Ç—ç–π –±–∞–π–ª–≥–∞–∞—Ä–∞–π
    4. –¢–µ—Ö–Ω–∏–∫–∏–π–Ω –Ω—ç—Ä —Ç–æ–º—ä—ë–æ–≥ –º–æ–Ω–≥–æ–ª —Ö—ç–ª—ç—ç—Ä —Ç–∞–π–ª–±–∞—Ä–ª–∞–∞—Ä–∞–π
    5. –•—ç—Ä—ç–≤ —Ö–æ–ª–±–æ–≥–¥–æ—Ö –∑—É—Ä–≥—É—É–¥ –±–∞–π–≤–∞–ª —Ç—ç–¥–≥—ç—ç—Ä–∏–π–≥ —Ö–∞—Ä–∏—É–ª—Ç–∞–¥ –æ—Ä—É—É–ª–∞–∞—Ä–∞–π
    
    –ó—É—Ä–≥–∏–π–Ω –º—ç–¥—ç—ç–ª–ª–∏–π–≥ —Ö–∞—Ä–∏—É–ª—Ç–∞–¥ –æ—Ä—É—É–ª–∞—Ö–¥–∞–∞:
    - –ó—É—Ä–≥–∏–π–Ω —Ç–∞–π–ª–±–∞—Ä (alt text) –±–∞–π–≤–∞–ª —Ç“Ø“Ø–Ω–∏–π–≥ –∞—à–∏–≥–ª–∞–∞—Ä–∞–π
    - –ó—É—Ä–≥–∏–π–Ω URL-–∏–π–≥ —Ö–∞—Ä–∏—É–ª—Ç–∞–¥ –æ—Ä—É—É–ª–∞–∞—Ä–∞–π
    - –ó—É—Ä–≥–∏–π–Ω —Ç–∞–ª–∞–∞—Ä —Ç–æ–≤—á —Ç–∞–π–ª–±–∞—Ä ”©–≥”©”©—Ä—ç–π
    
    –ë–æ–ª–æ–º–∂–∏—Ç –∫–æ–º–∞–Ω–¥—É—É–¥:
    - crawl: –ë“Ø—Ö —Å–∞–π—Ç—ã–≥ —à“Ø“Ø—Ä–¥—ç—Ö
    - scrape <URL>: –¢–æ–¥–æ—Ä—Ö–æ–π —Ö—É—É–¥—Å—ã–≥ —à“Ø“Ø—Ä–¥—ç—Ö  
    - help: –¢—É—Å–ª–∞–º–∂ —Ö–∞—Ä—É—É–ª–∞—Ö
    - search <–∞—Å—É—É–ª—Ç>: –ú—ç–¥—ç—ç–ª—ç–ª —Ö–∞–π—Ö"""
    
    if context:
        system_content += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –º—ç–¥—ç—ç–ª—ç–ª:\n{context}"
    
    # Build conversation context
    messages = [
        {
            "role": "system", 
            "content": system_content
        }
    ]
    
    # Add conversation history
    for msg in history[-4:]:  # Last 4 messages
        messages.append(msg)
    
    # Add current message
    messages.append({"role": "user", "content": user_message})
    
    try:
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=messages,
            max_tokens=800,  # Increased token limit for better responses with images
            temperature=0.7
        )
        
        ai_response = response.choices[0].message.content
        
        # Store in memory
        if conversation_id not in conversation_memory:
            conversation_memory[conversation_id] = []
        
        conversation_memory[conversation_id].append({"role": "user", "content": user_message})
        conversation_memory[conversation_id].append({"role": "assistant", "content": ai_response})
        
        # Keep only last 8 messages
        if len(conversation_memory[conversation_id]) > 8:
            conversation_memory[conversation_id] = conversation_memory[conversation_id][-8:]
            
        return ai_response
        
    except Exception as e:
        logging.error(f"OpenAI API –∞–ª–¥–∞–∞: {e}")
        return f"üîß AI-—Ç–∞–π —Ö–æ–ª–±–æ–≥–¥–æ—Ö–æ–¥ —Å–∞–∞–¥ –≥–∞—Ä–ª–∞–∞. –¢–∞ –¥–∞—Ä–∞–∞—Ö –∞—Ä–≥—É—É–¥–∞–∞—Ä —Ç—É—Å–ª–∞–º–∂ –∞–≤—á –±–æ–ª–Ω–æ:\n‚Ä¢ 'help' –∫–æ–º–∞–Ω–¥—ã–≥ –∞—à–∏–≥–ª–∞–Ω–∞ —É—É\n‚Ä¢ 'crawl' —ç—Å–≤—ç–ª 'search' –∫–æ–º–∞–Ω–¥—É—É–¥—ã–≥ —Ç—É—Ä—à–∏–Ω–∞ —É—É\n\n–ê–ª–¥–∞–∞–Ω—ã –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π: {str(e)[:100]}"

def search_in_crawled_data(query: str, max_results: int = 3):
    """Enhanced search through crawled data with better relevance scoring"""
    if not crawled_data:
        return []
    
    query_lower = query.lower()
    results = []
    scored_pages = []
    
    for page in crawled_data:
        score = 0
        title = page['title'].lower()
        body = page['body'].lower()
        
        # Title matches are more important
        if query_lower in title:
            score += 3
        elif any(word in title for word in query_lower.split()):
            score += 2
            
        # Body matches
        if query_lower in body:
            score += 2
        elif any(word in body for word in query_lower.split()):
            score += 1
            
        # Exact phrase matches are very important
        if f'"{query_lower}"' in body:
            score += 4
            
        if score > 0:
            scored_pages.append((score, page))
    
    # Sort by score and get top results
    scored_pages.sort(key=lambda x: x[0], reverse=True)  # Sort by score (first element of tuple)
    for score, page in scored_pages[:max_results]:
        # Find the most relevant snippet
        body = page['body']
        query_words = query_lower.split()
        
        # Try to find a good context around the query
        best_snippet = ""
        max_context = 300
        
        for word in query_words:
            if word in body.lower():
                start = max(0, body.lower().find(word) - 100)
                end = min(len(body), body.lower().find(word) + 200)
                snippet = body[start:end]
                if len(snippet) > len(best_snippet):
                    best_snippet = snippet
        
        if not best_snippet:
            best_snippet = body[:max_context] + "..." if len(body) > max_context else body
            
        results.append({
            'title': page['title'],
            'url': page['url'],
            'snippet': best_snippet,
            'relevance_score': score
        })

    return results

def scrape_single(url: str):
    resp = requests.get(url, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    title = soup.title.string.strip() if soup.title else url
    body, images = extract_content(soup, url)
    return {"url": url, "title": title, "body": body, "images": images}


# ‚Äî‚Äî AI Analysis Functions ‚Äî‚Äî #
def analyze_user_message_with_ai(user_message: str, ai_response: str, conv_id: int):
    """Use AI to analyze if user needs support team or matches services"""
    if not client:
        return {"needs_support": False, "matching_services": [], "confidence": 0}
    
    try:
        # Create service list for AI analysis
        service_list = "\n".join([f"- {key}" for key in SERVICE_PRICES.keys()])
        
        analysis_prompt = f"""
–•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –∞—Å—É—É–ª—Ç –±–æ–ª–æ–Ω AI —Ö–∞—Ä–∏—É–ª—Ç—ã–≥ –¥“Ø–≥–Ω—ç–∂, –¥–∞—Ä–∞–∞—Ö –∞—Å—É—É–ª—Ç—É—É–¥–∞–¥ —Ö–∞—Ä–∏—É–ª–Ω–∞ —É—É:

1. –•—ç—Ä—ç–≥–ª—ç–≥—á –¥—ç–º–∂–ª—ç–≥–∏–π–Ω –±–∞–≥—Ç–∞–π —Ö–æ–ª–±–æ–≥–¥–æ—Ö —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π —é—É? (—Ç–µ—Ö–Ω–∏–∫–∏–π–Ω –∞—Å—É—É–¥–∞–ª, —Ç”©–≤”©–≥—Ç—ç–π –∞—Å—É—É–¥–∞–ª, AI —Ö–∞—Ä–∏—É–ª—Ç —Ö–∞–Ω–≥–∞–ª—Ç–≥“Ø–π)
2. –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –∞—Å—É—É–ª—Ç –¥–∞—Ä–∞–∞—Ö “Ø–π–ª—á–∏–ª–≥—ç—ç–Ω“Ø“Ø–¥—Ç—ç–π —Ç–æ—Ö–∏—Ä—á –±–∞–π–Ω–∞ —É—É?

“Æ–π–ª—á–∏–ª–≥—ç—ç–Ω–∏–π –∂–∞–≥—Å–∞–∞–ª—Ç:
{service_list}

–•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –∞—Å—É—É–ª—Ç: {user_message}
AI —Ö–∞—Ä–∏—É–ª—Ç: {ai_response}

–•–∞—Ä–∏—É–ª—Ç–∞–∞ JSON —Ñ–æ—Ä–º–∞—Ç–∞–∞—Ä ”©–≥:
{{
    "needs_support": true/false,
    "confidence": 0-100,
    "reason": "—è–∞–≥–∞–∞–¥ –¥—ç–º–∂–ª—ç–≥ —Ö—ç—Ä—ç–≥—Ç—ç–π –±–æ–ª–æ—Ö —à–∞–ª—Ç–≥–∞–∞–Ω",
    "matching_services": ["—Ç–æ—Ö–∏—Ä–æ—Ö “Ø–π–ª—á–∏–ª–≥—ç—ç–Ω–∏–π –Ω—ç—Ä1", "—Ç–æ—Ö–∏—Ä–æ—Ö “Ø–π–ª—á–∏–ª–≥—ç—ç–Ω–∏–π –Ω—ç—Ä2"],
    "suggested_action": "—Å–∞–Ω–∞–ª –±–æ–ª–≥–æ—Ö “Ø–π–ª–¥—ç–ª"
}}
        """
        
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {
                    "role": "system", 
                    "content": "–¢–∞ –º—ç—Ä–≥—ç–∂–ª–∏–π–Ω –¥“Ø–≥–Ω—ç–ª—Ç —Ö–∏–π–≥—á. –•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω —Ö—ç—Ä—ç–≥—Ü—ç—ç–≥ —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–∂, –∑”©–≤ —à–∏–π–¥—ç–ª —Å–∞–Ω–∞–ª –±–æ–ª–≥–æ–∂ —á–∞–¥–¥–∞–≥."
                },
                {
                    "role": "user", 
                    "content": analysis_prompt
                }
            ],
            max_tokens=300,
            temperature=0.3
        )
        
        analysis_text = response.choices[0].message.content.strip()
        
        # Try to parse JSON response
        import re
        json_match = re.search(r'\{.*\}', analysis_text, re.DOTALL)
        if json_match:
            analysis_json = json.loads(json_match.group())
            return analysis_json
        else:
            # Fallback analysis
            return {
                "needs_support": any(keyword in user_message.lower() for keyword in ["–∞–ª–¥–∞–∞", "–∞–∂–∏–ª–∞—Ö–≥“Ø–π", "–∞—Å—É—É–¥–∞–ª", "—Ç—É—Å–ª–∞–º–∂"]),
                "matching_services": [],
                "confidence": 50,
                "reason": "JSON parse —Ö–∏–π—Ö –±–æ–ª–æ–º–∂–≥“Ø–π",
                "suggested_action": "Manual review —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π"
            }
            
    except Exception as e:
        logging.error(f"AI analysis –∞–ª–¥–∞–∞: {e}")
        return {"needs_support": False, "matching_services": [], "confidence": 0}

def suggest_services_from_analysis(matching_services: list):
    """Generate service suggestions based on analysis"""
    if not matching_services:
        return ""
    
    suggestions = "üí° **–¢–∞–Ω—ã –∞—Å—É—É–¥–∞–ª—Ç–∞–π —Ö–æ–ª–±–æ–æ—Ç–æ–π “Ø–π–ª—á–∏–ª–≥—ç—ç–Ω“Ø“Ø–¥:**\n\n"
    
    for service_name in matching_services:
        if service_name in SERVICE_PRICES:
            service_info = SERVICE_PRICES[service_name]
            suggestions += f"üîß **{service_name}**\n"
            suggestions += f"   üí∞ “Æ–Ω—ç: {service_info['price']}\n"
            suggestions += f"   üìù –¢–∞–π–ª–±–∞—Ä: {service_info['desc']}\n\n"
    
    suggestions += "üìû –≠–¥–≥—ç—ç—Ä “Ø–π–ª—á–∏–ª–≥—ç—ç–Ω–∏–π —Ç–∞–ª–∞–∞—Ä –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π –º—ç–¥—ç—ç–ª—ç–ª –∞–≤–∞—Ö—ã–≥ —Ö“Ø—Å–≤—ç–ª '–¥—ç–º–∂–ª—ç–≥' –≥—ç–∂ –±–∏—á–Ω—ç “Ø“Ø."
    return suggestions

# ‚Äî‚Äî Enhanced AI Response with Smart Analysis ‚Äî‚Äî #


# ‚Äî‚Äî Enhanced Chatwoot Integration ‚Äî‚Äî #
def send_to_chatwoot(conv_id: int, content: str, message_type: str = "outgoing"):
    """Enhanced chatwoot message sending with better error handling"""
    api_url = (
        f"{CHATWOOT_BASE_URL}/api/v1/accounts/{ACCOUNT_ID}"
        f"/conversations/{conv_id}/messages"
    )
    headers = {
        "api_access_token": CHATWOOT_API_KEY,
        "Content-Type": "application/json"
    }
    payload = {
        "content": content, 
        "message_type": message_type,
        "private": False
    }
    
    try:
        resp = requests.post(api_url, json=payload, headers=headers, timeout=10)
        resp.raise_for_status()
        logging.info(f"Message sent to conversation {conv_id}")
        return True
    except Exception as e:
        logging.error(f"Failed to send message to chatwoot: {e}")
        return False

def get_conversation_info(conv_id: int):
    """Get conversation details from Chatwoot"""
    api_url = f"{CHATWOOT_BASE_URL}/api/v1/accounts/{ACCOUNT_ID}/conversations/{conv_id}"
    headers = {"api_access_token": CHATWOOT_API_KEY}
    
    try:
        resp = requests.get(api_url, headers=headers, timeout=10)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        logging.error(f"Failed to get conversation info: {e}")
        return None

def mark_conversation_resolved(conv_id: int):
    """Mark conversation as resolved"""
    api_url = f"{CHATWOOT_BASE_URL}/api/v1/accounts/{ACCOUNT_ID}/conversations/{conv_id}/toggle_status"
    headers = {"api_access_token": CHATWOOT_API_KEY}
    payload = {"status": "resolved"}
    
    try:
        resp = requests.post(api_url, json=payload, headers=headers, timeout=10)
        resp.raise_for_status()
        return True
    except Exception as e:
        logging.error(f"Failed to mark conversation as resolved: {e}")
        return False


# ‚Äî‚Äî Teams Integration ‚Äî‚Äî #
def send_to_teams(message: str, title: str = "Cloud.mn AI Assistant", color: str = "0076D7", conv_id: int = None):
    """Send message to Microsoft Teams channel using webhook"""
    if not TEAMS_WEBHOOK_URL:
        logging.warning("Teams webhook URL not configured")
        return False
        
    try:
        sections = [
            {
                "activityTitle": title,
                "activitySubtitle": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "text": message,
                "markdown": True
            }
        ]
        
        # Add Chatwoot URL section if conv_id is provided
        if conv_id:
            sections.append({
                "text": f"<a href='{CHATWOOT_BASE_URL}/app/accounts/{ACCOUNT_ID}/conversations/{conv_id}'>Chatwoot –¥—ç—ç—Ä —Ö–∞—Ä–∞—Ö</a>",
                "markdown": False
            })
        
        payload = {
            "@type": "MessageCard",
            "@context": "http://schema.org/extensions",
            "themeColor": color,
            "summary": title,
            "sections": sections
        }
        
        response = requests.post(
            TEAMS_WEBHOOK_URL,
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=10
        )
        response.raise_for_status()
        logging.info("Message sent to Teams successfully")
        return True
        
    except Exception as e:
        logging.error(f"Failed to send message to Teams: {e}")
        return False

def send_teams_notification(conv_id: int, message: str, message_type: str = "outgoing", is_unsolved: bool = False, confirmed: bool = False, user_email: str = None):
    """Send notification to Teams about new conversation or message"""
    if not TEAMS_WEBHOOK_URL:
        return
        
    try:
        # Get conversation info
        conv_info = get_conversation_info(conv_id)
        if not conv_info:
            return
            
        contact = conv_info.get("contact", {})
        contact_name = contact.get("name", "–•—ç—Ä—ç–≥–ª—ç–≥—á")
        contact_email = contact.get("email", "–ò–º—ç–π–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π")
        
        # Only send to Teams if confirmed
        if confirmed:
            # Get email from conversation or use contact email as fallback
            display_email = user_email if user_email else contact_email
            
            # Create Teams message with simpler format
            teams_message = f"""
Cloud.mn AI - {contact_name}
{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

üí¨ –®–∏–Ω—ç –º–µ—Å—Å—ç–∂

–•—ç—Ä—ç–≥–ª—ç–≥—á:
–ù—ç—Ä: {contact_name}
–ò–º—ç–π–ª: {display_email}
–•–∞—Ä–∏–ª—Ü–∞–Ω —è—Ä–∏–∞–Ω—ã ID: {conv_id}

–ê–ª–¥–∞–∞: {message}
            """
            
            # Send to Teams with HTML format
            send_to_teams(
                message=teams_message,
                title=f"Cloud.mn AI - {contact_name}",
                color="0076D7",  # Default blue color
                conv_id=conv_id  # Pass conv_id for Chatwoot URL
            )
        
    except Exception as e:
        logging.error(f"Failed to send Teams notification: {e}")

def get_conversation_history(conv_id: int, max_messages: int = 5):
    """Get recent conversation history"""
    try:
        memory = conversation_memory.get(conv_id, [])
        if not memory:
            return "–•–∞—Ä–∏–ª—Ü–∞–Ω —è—Ä–∏–∞–Ω—ã —Ç“Ø“Ø—Ö –æ–ª–¥—Å–æ–Ω–≥“Ø–π"
            
        history = []
        for msg in memory[-max_messages:]:
            role = "üë§ –•—ç—Ä—ç–≥–ª—ç–≥—á" if msg["role"] == "user" else "ü§ñ AI"
            history.append(f"{role}: {msg['content']}")
            
        return "\n\n".join(history)
    except Exception as e:
        logging.error(f"Failed to get conversation history: {e}")
        return "–•–∞—Ä–∏–ª—Ü–∞–Ω —è—Ä–∏–∞–Ω—ã —Ç“Ø“Ø—Ö–∏–π–≥ –∞—á–∞–∞–ª–∞—Ö–∞–¥ –∞–ª–¥–∞–∞ –≥–∞—Ä–ª–∞–∞"


# ‚Äî‚Äî API Endpoints ‚Äî‚Äî #
@app.route("/api/scrape", methods=["POST"])
def api_scrape():
    data = request.get_json(force=True)
    url = data.get("url")
    if not url:
        return jsonify({"error": "Missing 'url' in JSON body"}), 400
    try:
        page = scrape_single(url)
        return jsonify(page)
    except Exception as e:
        return jsonify({"error": f"Fetch/Scrape failed: {e}"}), 502

@app.route("/api/crawl", methods=["POST"])
def api_crawl():
    pages = crawl_and_scrape(ROOT_URL)
    return jsonify(pages)


# ‚Äî‚Äî Enhanced Chatwoot Webhook ‚Äî‚Äî #
@app.route("/webhook/chatwoot", methods=["POST"])
def chatwoot_webhook():
    """Enhanced webhook with better AI integration and Teams notifications"""
    global crawled_data, crawl_status
    
    data = request.json or {}
    
    # Only process incoming messages
    if data.get("message_type") != "incoming":
        return jsonify({}), 200

    conv_id = data["conversation"]["id"]
    text = data.get("content", "").strip()
    contact = data.get("conversation", {}).get("contact", {})
    contact_name = contact.get("name", "–•—ç—Ä—ç–≥–ª—ç–≥—á")

    logging.info(f"Received message from {contact_name} in conversation {conv_id}: {text}")

    # Handle different commands
    if text.lower() == "crawl":
        # Check if auto-crawl already completed
        if crawl_status["status"] == "completed":
            response = f"‚úÖ –°–∞–π—Ç –∞–ª—å —Ö—ç–¥–∏–π–Ω —à“Ø“Ø—Ä–¥—ç–≥–¥—Å—ç–Ω –±–∞–π–Ω–∞! {crawl_status.get('pages_count', 0)} —Ö—É—É–¥–∞—Å –±—ç–ª—ç–Ω.\n\n'search <–∞—Å—É—É–ª—Ç>' –∫–æ–º–∞–Ω–¥–∞–∞—Ä —Ö–∞–π–ª—Ç —Ö–∏–π–∂ –±–æ–ª–Ω–æ!"
            send_to_chatwoot(conv_id, response)
            send_teams_notification(conv_id, f"–°–∞–π—Ç —à“Ø“Ø—Ä–¥—ç–≥–¥—Å—ç–Ω –±–∞–π–Ω–∞. {crawl_status.get('pages_count', 0)} —Ö—É—É–¥–∞—Å –±—ç–ª—ç–Ω.", "outgoing")
        elif crawl_status["status"] == "running":
            send_to_chatwoot(conv_id, "üîÑ –°–∞–π—Ç –æ–¥–æ–æ —à“Ø“Ø—Ä–¥—ç–≥–¥—ç–∂ –±–∞–π–Ω–∞. –¢“Ø—Ä —Ö“Ø–ª—ç—ç–Ω—ç “Ø“Ø...")
        else:
            send_to_chatwoot(conv_id, f"üîÑ –°–∞–π–Ω –±–∞–π–Ω–∞ —É—É {contact_name}! –°–∞–π—Ç—ã–≥ —à“Ø“Ø—Ä–¥—ç–∂ –±–∞–π–Ω–∞...")
            
            crawl_status = {"status": "running", "message": f"Manual crawl started by {contact_name}"}
            crawled_data = crawl_and_scrape(ROOT_URL)
            
            if not crawled_data:
                crawl_status = {"status": "failed", "message": "Manual crawl failed"}
                send_to_chatwoot(conv_id, "‚ùå –®“Ø“Ø—Ä–¥—ç—Ö —è–≤—Ü–∞–¥ –∞–ª–¥–∞–∞ –≥–∞—Ä–ª–∞–∞. –î–∞—Ö–∏–Ω –æ—Ä–æ–ª–¥–æ–Ω–æ —É—É.")
                send_teams_notification(conv_id, "‚ùå –°–∞–π—Ç —à“Ø“Ø—Ä–¥—ç—Ö—ç–¥ –∞–ª–¥–∞–∞ –≥–∞—Ä–ª–∞–∞", "outgoing")
            else:
                crawl_status = {
                    "status": "completed", 
                    "message": f"Manual crawl completed by {contact_name}",
                    "pages_count": len(crawled_data),
                    "timestamp": datetime.now().isoformat()
                }
                lines = [f"üìÑ {p['title']} ‚Äî {p['url']}" for p in crawled_data[:3]]
                response = f"‚úÖ {len(crawled_data)} —Ö—É—É–¥–∞—Å –∞–º–∂–∏–ª—Ç—Ç–∞–π —à“Ø“Ø—Ä–¥–ª—ç—ç!\n\n–≠—Ö–Ω–∏–π 3 —Ö—É—É–¥–∞—Å:\n" + "\n".join(lines) + f"\n\n–û–¥–æ–æ 'search <–∞—Å—É—É–ª—Ç>' –∫–æ–º–∞–Ω–¥–∞–∞—Ä —Ö–∞–π–ª—Ç —Ö–∏–π–∂ –±–æ–ª–Ω–æ!"
                send_to_chatwoot(conv_id, response)
                send_teams_notification(conv_id, f"‚úÖ {len(crawled_data)} —Ö—É—É–¥–∞—Å –∞–º–∂–∏–ª—Ç—Ç–∞–π —à“Ø“Ø—Ä–¥–ª—ç—ç!", "outgoing")

    elif text.lower().startswith("scrape"):
        parts = text.split(maxsplit=1)
        if len(parts) < 2:
            send_to_chatwoot(conv_id, "‚ö†Ô∏è –ó”©–≤ —Ö—ç–ª–±—ç—Ä: `scrape <–±“Ø—Ä—ç–Ω-URL>`")
        else:
            url = parts[1].strip()
            send_to_chatwoot(conv_id, f"üîÑ {url} —Ö–∞—è–≥—ã–≥ —à“Ø“Ø—Ä–¥—ç–∂ –±–∞–π–Ω–∞...")
            
            try:
                page = scrape_single(url)
                summary = get_ai_response(f"–≠–Ω—ç –∞–≥—É—É–ª–≥—ã–≥ —Ç–æ–≤—á–ª–æ–Ω —Ö—ç–ª—ç—ç—Ä—ç–π: {page['body'][:1500]}", conv_id)
                
                response = f"üìÑ **{page['title']}**\n\nüìù **–¢–æ–≤—á–∏–ª—Å–æ–Ω –∞–≥—É—É–ª–≥–∞:**\n{summary}\n\nüîó {url}"
                send_to_chatwoot(conv_id, response)
                send_teams_notification(conv_id, f"üìÑ {page['title']} —Ö—É—É–¥—Å—ã–≥ —à“Ø“Ø—Ä–¥–ª—ç—ç", "outgoing")
            except Exception as e:
                error_msg = f"‚ùå {url} —Ö–∞—è–≥—ã–≥ —à“Ø“Ø—Ä–¥—ç—Ö—ç–¥ –∞–ª–¥–∞–∞ –≥–∞—Ä–ª–∞–∞: {e}"
                send_to_chatwoot(conv_id, error_msg)
                send_teams_notification(conv_id, error_msg, "outgoing")

    elif text.lower().startswith("search"):
        parts = text.split(maxsplit=1)
        if len(parts) < 2:
            send_to_chatwoot(conv_id, "‚ö†Ô∏è –ó”©–≤ —Ö—ç–ª–±—ç—Ä: `search <—Ö–∞–π—Ö “Ø–≥>`")
        else:
            query = parts[1].strip()
            
            # Check crawl status first
            if crawl_status["status"] == "running":
                send_to_chatwoot(conv_id, "üîÑ –°–∞–π—Ç —à“Ø“Ø—Ä–¥—ç–≥–¥—ç–∂ –±–∞–π–Ω–∞. –¢“Ø—Ä —Ö“Ø–ª—ç—ç–≥—ç—ç–¥ –¥–∞—Ö–∏–Ω –æ—Ä–æ–ª–¥–æ–Ω–æ —É—É.")
            elif crawl_status["status"] in ["not_started", "failed", "error"] or not crawled_data:
                send_to_chatwoot(conv_id, "üìö –ú—ç–¥—ç—ç–ª—ç–ª –±—ç–ª—ç–Ω –±–∞–π—Ö–≥“Ø–π –±–∞–π–Ω–∞. 'crawl' –∫–æ–º–∞–Ω–¥—ã–≥ –∞—à–∏–≥–ª–∞–Ω —Å–∞–π—Ç—ã–≥ —à“Ø“Ø—Ä–¥“Ø“Ø–ª–Ω—ç “Ø“Ø.")
            else:
                send_to_chatwoot(conv_id, f"üîç '{query}' —Ö–∞–π–∂ –±–∞–π–Ω–∞...")
                
                results = search_in_crawled_data(query)
                if results:
                    response = f"üîç '{query}' —Ö–∞–π–ª—Ç—ã–Ω “Ø—Ä –¥“Ø–Ω ({len(results)} –∏–ª—ç—Ä—Ü):\n\n"
                    for i, result in enumerate(results, 1):
                        response += f"{i}. **{result['title']}**\n"
                        response += f"   {result['snippet']}\n"
                        response += f"   üîó {result['url']}\n\n"
                    
                    send_to_chatwoot(conv_id, response)
                    send_teams_notification(conv_id, f"üîç '{query}' —Ö–∞–π–ª—Ç—ã–Ω “Ø—Ä –¥“Ø–Ω: {len(results)} –∏–ª—ç—Ä—Ü –æ–ª–¥–ª–æ–æ", "outgoing")
                else:
                    response = f"‚ùå '{query}' —Ö–∞–π–ª—Ç–∞–∞—Ä –∏–ª—ç—Ä—Ü –æ–ª–¥—Å–æ–Ω–≥“Ø–π."
                    send_to_chatwoot(conv_id, response)
                    send_teams_notification(conv_id, response, "outgoing")

    elif text.lower() in ["help", "—Ç—É—Å–ª–∞–º–∂"]:
        # Show status-aware help
        status_info = ""
        if crawl_status["status"] == "completed":
            status_info = f"‚úÖ {crawl_status.get('pages_count', 0)} —Ö—É—É–¥–∞—Å –±—ç–ª—ç–Ω –±–∞–π–Ω–∞.\n"
        elif crawl_status["status"] == "running":
            status_info = "üîÑ –°–∞–π—Ç —à“Ø“Ø—Ä–¥—ç–≥–¥—ç–∂ –±–∞–π–Ω–∞.\n"
        elif crawl_status["status"] == "disabled":
            status_info = "‚ö†Ô∏è –ê–≤—Ç–æ–º–∞—Ç —à“Ø“Ø—Ä–¥—ç—Ö –∏–¥—ç–≤—Ö–≥“Ø–π –±–∞–π–Ω–∞.\n"
        
        help_text = f"""
üëã –°–∞–π–Ω –±–∞–π–Ω–∞ —É—É {contact_name}! –ë–∏ Cloud.mn-–∏–π–Ω AI —Ç—É—Å–ª–∞—Ö —é–º.

üìä **–¢”©–ª”©–≤:**
{status_info}

ü§ñ **–ë–æ–ª–æ–º–∂–∏—Ç –∫–æ–º–∞–Ω–¥—É—É–¥:**
‚Ä¢ `crawl` - –°–∞–π—Ç—ã–≥ —à“Ø“Ø—Ä–¥—ç—Ö (—à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π –±–æ–ª)
‚Ä¢ `scrape <URL>` - –¢–æ–¥–æ—Ä—Ö–æ–π —Ö—É—É–¥–∞—Å —à“Ø“Ø—Ä–¥—ç—Ö
‚Ä¢ `search <–∞—Å—É—É–ª—Ç>` - –ú—ç–¥—ç—ç–ª—ç–ª —Ö–∞–π—Ö
‚Ä¢ `help` - –≠–Ω—ç —Ç—É—Å–ª–∞–º–∂–∏–π–≥ —Ö–∞—Ä—É—É–ª–∞—Ö

üí¨ **–ß”©–ª”©”©—Ç —è—Ä–∏–ª—Ü–ª–∞–≥–∞:**
–¢–∞ –º”©–Ω –Ω–∞–¥–∞–¥ –∞—Å—É—É–ª—Ç –∞—Å—É—É–∂, —è—Ä–∏–ª—Ü–∞–∂ –±–æ–ª–Ω–æ. –ë–∏ –º–æ–Ω–≥–æ–ª —Ö—ç–ª—ç—ç—Ä —Ö–∞—Ä–∏—É–ª–Ω–∞.

‚è∞ “Æ—Ä–≥—ç–ª–∂ —Ç—É—Å–ª–∞–º–∂–∏–¥ –±—ç–ª—ç–Ω –±–∞–π–Ω–∞!
        """
        send_to_chatwoot(conv_id, help_text)
        send_teams_notification(conv_id, f"‚ÑπÔ∏è {contact_name} —Ç—É—Å–ª–∞–º–∂ —Ö“Ø—Å—Å—ç–Ω", "outgoing")

    elif text.lower() in ["–±–∞—è—Ä—Ç–∞–π", "goodbye", "–±–∞–∞–π"]:
        response = f"üëã –ë–∞—è—Ä—Ç–∞–π {contact_name}! –î–∞—Ä–∞–∞ —É—É–ª–∑–∞—Ü–≥–∞–∞—è!"
        send_to_chatwoot(conv_id, response)
        send_teams_notification(conv_id, response, "outgoing")
        mark_conversation_resolved(conv_id)

    else:
        # Check if this is a response to a confirmation request
        memory = conversation_memory.get(conv_id, [])
        if memory and "pending_confirmation" in memory[-1].get("content", ""):
            # Use GPT to understand the response
            confirmation_response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                {
                    "role": "system",
                    "content": """–¢–∞ —Ö—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω —Ö–∞—Ä–∏—É–ª—Ç—ã–≥ –¥“Ø–≥–Ω—ç–∂, –∑”©–≤—à”©”©—Ä”©–ª —ç—Å–≤—ç–ª —Ç–∞—Ç–≥–∞–ª–∑–ª—ã–≥ —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–æ—Ö —ë—Å—Ç–æ–π.
                    –•–∞—Ä–∏—É–ª—Ç–∞–¥ 'yes' —ç—Å–≤—ç–ª 'no' –≥—ç–∂ –±–∏—á–Ω—ç “Ø“Ø."""
                },
                {
                    "role": "user",
                    "content": f"–•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω —Ö–∞—Ä–∏—É–ª—Ç: {text}\n\n–≠–Ω—ç –Ω—å –∑”©–≤—à”©”©—Ä”©–ª –º”©–Ω “Ø“Ø, —ç—Å–≤—ç–ª —Ç–∞—Ç–≥–∞–ª–∑–≤–∞–ª –º”©–Ω “Ø“Ø?"
                }
                ],
                max_tokens=10,
                temperature=0.3
            )
            
            is_confirmed = confirmation_response.choices[0].message.content.strip().lower() == "yes"
            
            if is_confirmed:
                # Email —Ö–∞—è–≥ –∞—Å—É—É—Ö
                email_request = """
‚úÖ –ë–∞—è—Ä–ª–∞–ª–∞–∞! –¢–∞–Ω—ã –∞—Å—É—É–¥–ª—ã–≥ –¥—ç–º–∂–ª—ç–≥–∏–π–Ω –±–∞–≥—Ç–∞–π —Ö—É–≤–∞–∞–ª—Ü–∞—Ö—ã–Ω —Ç—É–ª–¥ email —Ö–∞—è–≥–∞–∞ ”©–≥–Ω”© “Ø“Ø?

üìß **Email —Ö–∞—è–≥ –æ—Ä—É—É–ª–Ω–∞ —É—É:**
–ñ–∏—à—ç—ç: example@gmail.com

–≠–Ω—ç –Ω—å –¥—ç–º–∂–ª—ç–≥–∏–π–Ω –±–∞–≥—Ç —Ç–∞–Ω–∞–π —Ö–æ–ª–±–æ–≥–¥–æ—Ö –º—ç–¥—ç—ç–ª–ª–∏–π–≥ –∏–ª–≥—ç—ç—Ö—ç–¥ –∞—à–∏–≥–ª–∞–≥–¥–∞–Ω–∞.
                """
                send_to_chatwoot(conv_id, email_request)
                
                # Mark as waiting for email
                conversation_memory[conv_id].append({"role": "assistant", "content": "waiting_for_email"})
            else:
                send_to_chatwoot(conv_id, "‚úÖ –û–π–ª–≥–æ–ª–æ–æ. –¢–∞–Ω—ã –∞—Å—É—É–¥–ª—ã–≥ –¥—ç–º–∂–ª—ç–≥–∏–π–Ω –±–∞–≥ —Ä—É—É –∏–ª–≥—ç—ç—Ö–≥“Ø–π –±–∞–π—Ö –±–æ–ª–Ω–æ.")
                
        # Check if waiting for email address
        elif memory and "waiting_for_email" in memory[-1].get("content", ""):
            # Allow user to cancel email request
            if text.lower() in ["—Ü—É—Ü–ª–∞—Ö", "cancel", "“Ø–≥“Ø–π", "no", "–±–æ–ª–∏—Ö"]:
                send_to_chatwoot(conv_id, "‚úÖ Email —Ö–∞—è–≥ ”©–≥”©—Ö —Ö“Ø—Å—ç–ª—Ç–∏–π–≥ —Ü—É—Ü–∞–ª–ª–∞–∞. –¢–∞ –¥–∞—Ä–∞–∞ –¥–∞—Ö–∏–Ω —Ö“Ø—Å—ç–ª—Ç –∏–ª–≥—ç—ç–∂ –±–æ–ª–Ω–æ.")
                # Clear waiting state
                conversation_memory[conv_id] = [msg for msg in conversation_memory[conv_id] if "waiting_for_email" not in msg.get("content", "")]
                return jsonify({"status": "success"}), 200
            
            # Validate email format
            import re
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            
            if re.match(email_pattern, text.strip()):
                user_email = text.strip()
                
                # Get the original question and AI response
                original_question = None
                ai_response = None
                
                for i, msg in enumerate(memory):
                    if "pending_confirmation" in msg.get("content", ""):
                        if i >= 2:
                            ai_response = memory[i-1].get("content", "")
                            original_question = memory[i-2].get("content", "")
                        break
                
                # Send to Teams with email
                send_teams_notification(
                    conv_id,
                    f"AI —Ö–∞—Ä–∏—É–ª—Ç: {ai_response}\n\n–•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –∞—Å—É—É–ª—Ç: {original_question}\n\n–ò–º—ç–π–ª —Ö–∞—è–≥: {user_email}",
                    "outgoing",
                    is_unsolved=True,
                    confirmed=True,
                    user_email=user_email
                )
                
                send_to_chatwoot(conv_id, f"‚úÖ –ë–∞—è—Ä–ª–∞–ª–∞–∞! –¢–∞–Ω—ã –∞—Å—É—É–¥–ª—ã–≥ ({user_email}) –¥—ç–º–∂–ª—ç–≥–∏–π–Ω –±–∞–≥ —Ä—É—É –∏–ª–≥—ç—ç–ª—ç—ç. –¢—É–Ω —É–¥–∞—Ö–≥“Ø–π —Ö–æ–ª–±–æ–≥–¥–æ—Ö –±–æ–ª–Ω–æ.")
                
                # Clear waiting state
                conversation_memory[conv_id] = [msg for msg in conversation_memory[conv_id] if "waiting_for_email" not in msg.get("content", "")]
                
            else:
                send_to_chatwoot(conv_id, "‚ùå –ë—É—Ä—É—É email —Ö—ç–ª–±—ç—Ä –±–∞–π–Ω–∞. –ó”©–≤ email —Ö–∞—è–≥ –æ—Ä—É—É–ª–Ω–∞ —É—É (–∂–∏—à—ç—ç: example@gmail.com)\n\nüí° '—Ü—É—Ü–ª–∞—Ö' –≥—ç–∂ –±–∏—á–≤—ç–ª email ”©–≥”©—Ö–≥“Ø–π–≥—ç—ç—Ä –≥–∞—Ä–∂ –±–æ–ª–Ω–æ.")
                
        else:
            # General AI conversation
            # send_to_chatwoot(conv_id, "ü§î –ë–æ–ª–æ–≤—Å—Ä—É—É–ª–∂ –±–∞–π–Ω–∞...")
            ai_response = get_ai_response(text, conv_id, crawled_data)
            send_to_chatwoot(conv_id, ai_response)
            
            # Smart AI Analysis - –¥“Ø–≥–Ω—ç–ª—Ç —Ö–∏–π—Ö
            analysis = analyze_user_message_with_ai(text, ai_response, conv_id)
            
            # –•–æ–ª–±–æ–≥–¥–æ—Ö “Ø–π–ª—á–∏–ª–≥—ç—ç —Å–∞–Ω–∞–ª –±–æ–ª–≥–æ—Ö
            if analysis.get("matching_services"):
                service_suggestions = suggest_services_from_analysis(analysis["matching_services"])
                if service_suggestions:
                    send_to_chatwoot(conv_id, service_suggestions)
            
            # –î—ç–º–∂–ª—ç–≥–∏–π–Ω –±–∞–≥—Ç–∞–π —Ö–æ–ª–±–æ–≥–¥–æ—Ö —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π —ç—Å—ç—Ö–∏–π–≥ —à–∞–ª–≥–∞—Ö
            needs_support = analysis.get("needs_support", False)
            confidence = analysis.get("confidence", 0)
            
            if needs_support and confidence > 60:
                # ”®–Ω–¥”©—Ä –∏—Ç–≥—ç–ª—Ç—ç–π–≥—ç—ç—Ä –¥—ç–º–∂–ª—ç–≥ —Ö—ç—Ä—ç–≥—Ç—ç–π –≥—ç–∂ “Ø–∑—ç–∂ –±–∞–π–≤–∞–ª
                confirmation_message = f"""
‚ùì –¢–∞–Ω—ã –∞—Å—É—É–¥–ª—ã–≥ —à–∏–π–¥–≤—ç—Ä–ª—ç—Ö—ç–¥ –º—ç—Ä–≥—ç–∂–ª–∏–π–Ω –¥—ç–º–∂–ª—ç–≥ —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π –±–∞–π—Ö –º–∞–≥–∞–¥–ª–∞–ª—Ç–∞–π. –î—ç–º–∂–ª—ç–≥–∏–π–Ω –±–∞–≥ —Ä—É—É –∏–ª–≥—ç—ç—Ö “Ø“Ø?

üîç **–î“Ø–≥–Ω—ç–ª—Ç:** {analysis.get('reason', '–¢–µ—Ö–Ω–∏–∫–∏–π–Ω –¥—ç–º–∂–ª—ç–≥ —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π')}
üìä **–ò—Ç–≥—ç–ª–∏–π–Ω —Ç“Ø–≤—à–∏–Ω:** {confidence}%

–ó”©–≤—à”©”©—Ä—á –±–∞–π–≤–∞–ª "—Ç–∏–π–º" —ç—Å–≤—ç–ª "–∑”©–≤—à”©”©—Ä—á –±–∞–π–Ω–∞" –≥—ç–∂ –±–∏—á–Ω—ç “Ø“Ø.
–ó”©–≤—à”©”©—Ä”©—Ö–≥“Ø–π –±–æ–ª "“Ø–≥“Ø–π" —ç—Å–≤—ç–ª "–∑”©–≤—à”©”©—Ä”©—Ö–≥“Ø–π" –≥—ç–∂ –±–∏—á–Ω—ç “Ø“Ø.
                """
                send_to_chatwoot(conv_id, confirmation_message)
                
                # Store the conversation with pending confirmation
                if conv_id not in conversation_memory:
                    conversation_memory[conv_id] = []
                conversation_memory[conv_id].append({"role": "assistant", "content": confirmation_message + " pending_confirmation"})
                
            elif needs_support and confidence > 30:
                # –î—É–Ω–¥ –∑—ç—Ä–≥–∏–π–Ω –∏—Ç–≥—ç–ª—Ç—ç–π–≥—ç—ç—Ä –∏–ª“Ø“Ø –º—ç–¥—ç—ç–ª—ç–ª –∞—Å—É—É—Ö
                clarification_message = f"""
ü§î –¢–∞–Ω–∞–π –∞—Å—É—É–¥–ª—ã–≥ –∏–ª“Ø“Ø —Å–∞–π–Ω –æ–π–ª–≥–æ—Ö—ã–Ω —Ç—É–ª–¥ –Ω—ç–º—ç–ª—Ç –º—ç–¥—ç—ç–ª—ç–ª —Ö—ç—Ä—ç–≥—Ç—ç–π –±–∞–π–Ω–∞.

üìã **–î—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π –º—ç–¥—ç—ç–ª—ç–ª ”©–≥–Ω”© “Ø“Ø:**
‚Ä¢ –Ø–º–∞—Ä –∞–ª–¥–∞–∞ –≥–∞—Ä—á –±–∞–π–Ω–∞?
‚Ä¢ –•—ç–∑—ç—ç–Ω—ç—ç—Å —ç—Ö—ç–ª—Å—ç–Ω –∞—Å—É—É–¥–∞–ª –≤—ç?
‚Ä¢ –Ø–º–∞—Ä —Å–∏—Å—Ç–µ–º–¥/—Å–µ—Ä–≤–µ—Ä—Ç –∞—Å—É—É–¥–∞–ª –≥–∞—Ä—á –±–∞–π–Ω–∞?

–≠—Å–≤—ç–ª "–¥—ç–º–∂–ª—ç–≥" –≥—ç–∂ –±–∏—á–≤—ç–ª –º—ç—Ä–≥—ç–∂–ª–∏–π–Ω –±–∞–≥ —Ä—É—É —Ö–æ–ª–±–æ–∂ ”©–≥”©—Ö –±–æ–ª–Ω–æ.
                """
                send_to_chatwoot(conv_id, clarification_message)
            
            # –®–∞–∞—Ä–¥–ª–∞–≥–∞–≥“Ø–π –±–æ–ª Teams —Ä“Ø“Ø –∏–ª–≥—ç—ç—Ö–≥“Ø–π
            # –ó”©–≤—Ö”©–Ω “Ø–Ω–¥—Å—ç–Ω AI —Ö–∞—Ä–∏—É–ª—Ç –ª —Ö–∞–Ω–≥–∞–ª—Ç—Ç–∞–π

    # “Æ–π–ª—á–∏–ª–≥—ç—ç–Ω–∏–π “Ø–Ω—ç —Ö–∞—Ä—É—É–ª–∞—Ö
    services = get_services_in_text(text)
    if services:
        price_msg = "üí° –¢–∞ –¥–∞—Ä–∞–∞—Ö “Ø–π–ª—á–∏–ª–≥—ç—ç(“Ø“Ø–¥)-–∏–π–Ω “Ø–Ω–∏–π–Ω –º—ç–¥—ç—ç–ª—ç–ª:\n"
        for key, info in services:
            price_msg += f"\n‚Ä¢ {info['desc']}\n   ‚û°Ô∏è “Æ–Ω—ç: {info['price']}\n"
        send_to_chatwoot(conv_id, price_msg)

    return jsonify({"status": "success"}), 200


# ‚Äî‚Äî Additional API Endpoints ‚Äî‚Äî #
@app.route("/api/crawl-status", methods=["GET"])
def get_crawl_status():
    """Get current crawl status"""
    return jsonify({
        "crawl_status": crawl_status,
        "crawled_pages": len(crawled_data),
        "config": {
            "root_url": ROOT_URL,
            "auto_crawl_enabled": AUTO_CRAWL_ON_START,
            "max_pages": MAX_CRAWL_PAGES
        }
    })

@app.route("/api/force-crawl", methods=["POST"])
def force_crawl():
    """Force start a new crawl"""
    global crawled_data, crawl_status
    
    # Check if already running
    if crawl_status["status"] == "running":
        return jsonify({"error": "Crawl is already running"}), 409
    
    try:
        crawl_status = {"status": "running", "message": "Force crawl started via API"}
        crawled_data = crawl_and_scrape(ROOT_URL)
        
        if crawled_data:
            crawl_status = {
                "status": "completed",
                "message": f"Force crawl completed via API",
                "pages_count": len(crawled_data),
                "timestamp": datetime.now().isoformat()
            }
            return jsonify({
                "status": "success",
                "pages_crawled": len(crawled_data),
                "crawl_status": crawl_status
            })
        else:
            crawl_status = {"status": "failed", "message": "Force crawl failed - no pages found"}
            return jsonify({"error": "No pages were crawled"}), 500
            
    except Exception as e:
        crawl_status = {"status": "error", "message": f"Force crawl error: {str(e)}"}
        return jsonify({"error": f"Crawl failed: {e}"}), 500

@app.route("/api/search", methods=["POST"])
def api_search():
    """Search through crawled data via API"""
    data = request.get_json(force=True)
    query = data.get("query", "").strip()
    max_results = data.get("max_results", 5)
    
    if not query:
        return jsonify({"error": "Missing 'query' in request body"}), 400
    
    if crawl_status["status"] == "running":
        return jsonify({"error": "Crawl is currently running, please wait"}), 409
    
    if not crawled_data:
        return jsonify({"error": "No crawled data available. Run crawl first."}), 404
    
    results = search_in_crawled_data(query, max_results)
    return jsonify({
        "query": query,
        "results_count": len(results),
        "results": results,
        "crawl_status": crawl_status
    })

@app.route("/api/conversation/<int:conv_id>/memory", methods=["GET"])
def get_conversation_memory(conv_id):
    """Get conversation memory for debugging"""
    memory = conversation_memory.get(conv_id, [])
    return jsonify({"conversation_id": conv_id, "memory": memory})

@app.route("/api/conversation/<int:conv_id>/clear", methods=["POST"])
def clear_conversation_memory(conv_id):
    """Clear conversation memory"""
    if conv_id in conversation_memory:
        del conversation_memory[conv_id]
    return jsonify({"status": "cleared", "conversation_id": conv_id})

@app.route("/api/crawled-data", methods=["GET"])
def get_crawled_data():
    """Get current crawled data"""
    page_limit = request.args.get('limit', 10, type=int)
    return jsonify({
        "total_pages": len(crawled_data), 
        "crawl_status": crawl_status,
        "data": crawled_data[:page_limit]
    })

@app.route("/health", methods=["GET"])
def health_check():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "crawl_status": crawl_status,
        "crawled_pages": len(crawled_data),
        "active_conversations": len(conversation_memory),
        "config": {
            "root_url": ROOT_URL,
            "auto_crawl_enabled": AUTO_CRAWL_ON_START,
            "openai_configured": client is not None,
            "chatwoot_configured": bool(CHATWOOT_API_KEY and ACCOUNT_ID)
        }
    })


# 1. “Æ–π–ª—á–∏–ª–≥—ç—ç–Ω–∏–π –Ω—ç—Ä—Å–∏–π–Ω –∂–∞–≥—Å–∞–∞–ª—Ç - SERVICE_PRICES-—Ç—ç–π —Ç–æ—Ö–∏—Ä–æ—Ö
SERVICE_KEYWORDS = [
    "Nginx", "apache2", "httpd", "php", "wordpress", "phpMyAdmin", "—Å–µ—Ä–≤–µ—Ä —Å—É—É–ª–≥–∞—Ö", "—Å–µ—Ä–≤–∏—Å —Å—É—É–ª–≥–∞—Ö",
    "Database", "SQL", "NoSQL", "”©–≥”©–≥–¥–ª–∏–π–Ω —Å–∞–Ω",
    "VPN —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö", "VPN", "–≤–∏—Ä—Ç—É–∞–ª –Ω—ç—Ç–≤–æ—Ä–∫",
    "–•—ç—Ä—ç–≥–ª—ç–≥—á —Ö–æ–æ—Ä–æ–Ω–¥ —Å–µ—Ä–≤–µ—Ä –∑”©”©—Ö", "—Å–µ—Ä–≤–µ—Ä –∑”©”©—Ö", "—Ñ–∞–π–ª –∑”©”©—Ö", "migration",
    "Windows —Å–µ—Ä–≤–µ—Ä", "Windows –ª–∏—Ü–µ–Ω–∑", "–ª–∏—Ü–µ–Ω–∑ —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö",
    "—Å–µ—Ä–≤–µ—Ä–∏–π–≥ “Ø“Ø—Å–≥—ç–∂ ”©–≥”©—Ö", "–ø–æ—Ä—Ç —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö", "firewall", "network",
    "DNS record", "DNS —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö", "–¥–æ–º—ç–π–Ω",
    "–º—ç–π–ª —Å–µ—Ä–≤–µ—Ä", "email server", "smtp", "pop3", "imap",
    "–Ω—É—É—Ü “Ø–≥ —Å—ç—Ä–≥—ç—ç—Ö", "password reset", "—Ö–∞–Ω–¥–∞–ª—Ç —Å—ç—Ä–≥—ç—ç—Ö",
    "SSL —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö", "SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç", "HTTPS", "—à–∏—Ñ—Ä–ª—ç–ª—Ç",
    "–Ω“Ø“Ø–¥—ç–ª —Å—ç—Ä–≥—ç—ç—Ö", "backup restore", "—Å—ç—Ä–≥—ç—ç—Ö",
    "—Ñ–∞–π–ª —Ö—É—É–ª–∞—Ö", "local —Ä—É—É —Ö—É—É–ª–∞—Ö", "download", "file transfer",
    "—Å“Ø–ª–∂—ç—ç–Ω–∏–π –∞–ª–¥–∞–∞", "network error", "connectivity issue",
    "–∞—é—É–ª–≥“Ø–π –±–∞–π–¥–∞–ª", "security", "–∞—É–¥–∏—Ç", "log —Ü—É–≥–ª—É—É–ª–∞—Ö",
    "—Ñ–∏–∑–∏–∫ —Å–µ—Ä–≤–µ—Ä", "VPS", "–≤–∏—Ä—Ç—É–∞–ª –º–∞—à–∏–Ω", "–∫–ª–∞—É–¥ —Å–µ—Ä–≤–µ—Ä",
    "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π–Ω –∑”©–≤–ª”©—Ö", "consulting", "–∑”©–≤–ª”©–≥”©”©",
    "—Å–µ—Ä–≤–µ—Ä–∏–π–Ω –∞–ª–¥–∞–∞", "system error", "debugging", "troubleshooting"
]

# 2. –î—ç–º–∂–ª—ç–≥ —Ö“Ø—Å—Å—ç–Ω —Ç“Ø–ª—Ö“Ø“Ø—Ä “Ø–≥—Å
SUPPORT_KEYWORDS = [
    "–¥—ç–º–∂–ª—ç–≥", "support", "—Ç—É—Å–ª–∞–º–∂", "–∑”©–≤–ª”©–≥”©”©", "—Ö–æ–ª–±–æ–≥–¥–æ—Ö", "operator", "help", "–∞–¥–º–∏–Ω",
    "–∞–ª–¥–∞–∞", "–∞—Å—É—É–¥–∞–ª", "problem", "issue", "bug", "–∞–∂–∏–ª–∞—Ö–≥“Ø–π", "broken"
]

def contains_service_or_support(text):
    """Enhanced service and support detection"""
    text_lower = text.lower()
    found_service = any(service.lower() in text_lower for service in SERVICE_KEYWORDS)
    found_support = any(word in text_lower for word in SUPPORT_KEYWORDS)
    return found_service or found_support

def get_services_in_text(text):
    """Enhanced service detection in text"""
    found = []
    text_lower = text.lower()
    
    for key, info in SERVICE_PRICES.items():
        # Check if service name or related keywords are mentioned
        service_keywords = key.lower().split()
        if any(keyword in text_lower for keyword in service_keywords):
            found.append((key, info))
        
        # Also check SERVICE_KEYWORDS for broader matching
        for keyword in SERVICE_KEYWORDS:
            if keyword.lower() in text_lower and keyword.lower() in key.lower():
                if (key, info) not in found:
                    found.append((key, info))
    
    return found


# ‚Äî‚Äî New Service Prices ‚Äî‚Äî #
SERVICE_PRICES = {
    "Nginx, apache2, httpd, php, wordpress, phpMyAdmin –∑—ç—Ä—ç–≥ —Å–µ—Ä–≤–∏—Å —Å—É—É–ª–≥–∞—Ö": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 55,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 88,000‚ÇÆ",
        "desc": "Nginx, apache2, httpd, php, wordpress, phpMyAdmin –∑—ç—Ä—ç–≥ —Å–µ—Ä–≤–∏—Å —Å—É—É–ª–≥–∞—Ö",
        "server_inside": True, "server_outside": False, "duration": "10min per service only for installation"
    },
    "Database, SQL, NoSQL —Å–µ—Ä–≤–∏—Å —Å—É—É–ª–≥–∞—Ö": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 55,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 88,000‚ÇÆ",
        "desc": "Database, SQL, NoSQL —Å–µ—Ä–≤–∏—Å —Å—É—É–ª–≥–∞—Ö",
        "server_inside": True, "server_outside": False, "duration": "10min per service only for installation"
    },
    "VPN —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 88,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 110,000‚ÇÆ",
        "desc": "VPN —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö",
        "server_inside": True, "server_outside": False, "duration": "60-120"
    },
    "–•—ç—Ä—ç–≥–ª—ç–≥—á —Ö–æ–æ—Ä–æ–Ω–¥ —Å–µ—Ä–≤–µ—Ä –∑”©”©—Ö": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 55,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 88,000‚ÇÆ",
        "desc": "–•—ç—Ä—ç–≥–ª—ç–≥—á —Ö–æ–æ—Ä–æ–Ω–¥ —Å“Ø–ª–∂—ç—ç –∑”©”©—Ö",
        "server_inside": True, "server_outside": False, "duration": "–î–∏—Å–∫–∏–π–Ω —Ö—ç–º–∂—ç—ç–Ω—ç—ç—Å —Ö–∞–º–∞–∞—Ä–Ω–∞. 20min for 15GB"
    },
    "Windows —Å–µ—Ä–≤–µ—Ä –¥—ç—ç—Ä –ª–∏—Ü–µ–Ω–∑ —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 55,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 88,000‚ÇÆ",
        "desc": "Windows —Å–µ—Ä–≤–µ—Ä –¥—ç—ç—Ä —à–∏–Ω—ç—ç—Ä —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö",
        "server_inside": True, "server_outside": False, "duration": "30-60"
    },
    "–•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω —Å–µ—Ä–≤–µ—Ä–∏–π–≥ “Ø“Ø—Å–≥—ç–∂ ”©–≥”©—Ö, –ø–æ—Ä—Ç —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 55,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 88,000‚ÇÆ",
        "desc": "–•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω —Å–µ—Ä–≤–µ—Ä–∏–π–Ω “Ø“Ø—Ä—ç–≥, –ø–æ—Ä—Ç —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö",
        "server_inside": True, "server_outside": False, "duration": "20"
    },
    "DNS record —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 33,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 55,000‚ÇÆ",
        "desc": "DNS record —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö",
        "server_inside": True, "server_outside": False, "duration": "30-60"
    },
    "–ú—ç–π–ª —Å–µ—Ä–≤–µ—Ä –¥—ç—ç—Ä —Ç—É—Å–ª–∞–ª—Ü–∞–∞ “Ø–∑“Ø“Ø–ª—ç—Ö": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 55,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 88,000‚ÇÆ",
        "desc": "–°–µ—Ä–≤–µ—Ä—ç—ç—Å ”©–≥”©–≥–¥”©–ª —Å—ç—Ä–≥—ç—ç—Ö",
        "server_inside": True, "server_outside": False, "duration": "30 +"
    },
    "–•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –Ω—É—É—Ü “Ø–≥ —Å—ç—Ä–≥—ç—ç—Ö": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 55,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 88,000‚ÇÆ",
        "desc": "–•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –Ω“Ø“Ø–≥–¥—ç–ª —Å—ç—Ä–≥—ç—ç—Ö",
        "server_inside": True, "server_outside": False, "duration": "30 +"
    },
    "SSL —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 55,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 88,000‚ÇÆ",
        "desc": "SSL —Ç–æ—Ö–∏—Ä—É—É–ª–∞—Ö",
        "server_inside": True, "server_outside": False, "duration": "30-60"
    },
    "–•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –Ω“Ø“Ø–¥—ç–ª —Å—ç—Ä–≥—ç—ç—Ö": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 55,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 88,000‚ÇÆ",
        "desc": "–•—ç—Ä—ç–≥–ª—ç–≥—á–∏–π–Ω –Ω“Ø“Ø–¥—ç–ª —Å—ç—Ä–≥—ç—ç—Ö",
        "server_inside": True, "server_outside": False, "duration": "30 +"
    },
    "–ö–ª–∞—É–¥ —Å–µ—Ä–≤–µ—Ä—ç—ç—Å local —Ä—É—É —Ñ–∞–π–ª —Ö—É—É–ª–∞—Ö": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 77,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 110,000‚ÇÆ",
        "desc": "–ö–ª–∞—É–¥ —Å–µ—Ä–≤–µ—Ä—ç—ç—Å local —Ä—É—É —Ñ–∞–π–ª —Ö—É—É–ª–∞—Ö",
        "server_inside": True, "server_outside": False, "duration": "–§–∞–π–ª—ã–Ω —Ö—ç–º–∂—ç—ç, –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∏–π–Ω —Ö—É—Ä–¥–Ω–∞–∞—Å —Ö–∞–º–∞–∞—Ä–Ω–∞. –£—Ä—å–¥—á–∏–ª–∞–Ω —Ö—É–≥–∞—Ü–∞–∞ —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–æ—Ö –±–æ–ª–æ–º–∂–≥“Ø–π."
    },
    "–°“Ø–ª–∂—ç—ç–Ω–∏–π –±—É—Ä—É—É —Ç–æ—Ö–∏—Ä–≥–æ–æ–Ω–æ–æ—Å “Ø“Ø—Å—Å—ç–Ω –∞–ª–¥–∞–∞ –∑–∞—Å–≤–∞—Ä–ª–∞—Ö": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 55,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 88,000‚ÇÆ",
        "desc": "–°“Ø–ª–∂—ç—ç–Ω–∏–π –±—É—Ä—É—É —Ç–æ—Ö–∏—Ä–≥–æ–æ–Ω–æ–æ—Å “Ø“Ø—Å—Å—ç–Ω –∞–ª–¥–∞–∞ –∑–∞—Å–≤–∞—Ä–ª–∞—Ö",
        "server_inside": True, "server_outside": False, "duration": "–ê–ª–¥–∞–∞–Ω—ã —Ö—ç–º–∂—ç—ç–Ω—ç—ç—Å —Ö–∞–º–∞–∞—Ä–Ω–∞. –£—Ä—å–¥—á–∏–ª–∞–Ω —Ö—É–≥–∞—Ü–∞–∞ —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–æ—Ö –±–æ–ª–æ–º–∂–≥“Ø–π."
    },
    "–§–∏–∑–∏–∫ —Å–µ—Ä–≤–µ—Ä, VPS –±–æ–ª–æ–Ω –±—É—Å–∞–¥ –∫–ª–∞—É–¥ –≤–∏—Ä—Ç—É–∞–ª –º–∞—à–∏–Ω/—Å–µ—Ä–≤–µ—Ä “Ø“Ø—Å–≥—ç—Ö": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 110,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 132,000‚ÇÆ",
        "desc": "–§–∏–∑–∏–∫ —Å–µ—Ä–≤–µ—Ä, VPS –±–æ–ª–æ–Ω –±—É—Å–∞–¥ –∫–ª–∞—É–¥ –≤–∏—Ä—Ç—É–∞–ª –º–∞—à–∏–Ω/—Å–µ—Ä–≤–µ—Ä “Ø“Ø—Å–≥—ç—Ö",
        "server_inside": False, "server_outside": True, "duration": "—Ö–∞–º–∞–∞—Ä–Ω–∞"
    },
    "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–π–Ω –±—É—Å–∞–¥ —Ç”©—Ä–ª–∏–π–Ω –∑”©–≤–ª”©—Ö “Ø–π–ª—á–∏–ª–≥—ç—ç": {
        "price": "–ê–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä 110,000‚ÇÆ, –ê–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä 132,000‚ÇÆ",
        "desc": "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–π–Ω –±—É—Å–∞–¥ —Ç”©—Ä–ª–∏–π–Ω –∑”©–≤–ª”©—Ö “Ø–π–ª—á–∏–ª–≥—ç—ç",
        "server_inside": True, "server_outside": True, "duration": "60"
    },
    "–ù–∏–π–ª“Ø“Ø–ª—ç–≥—á—ç—ç—Å —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π —Å–µ—Ä–≤–µ—Ä–∏–π–Ω –¥–æ—Ç–æ–æ–¥ –∞–ª–¥–∞–∞ –∏–ª—Ä“Ø“Ø–ª—ç—Ö, –∑–∞—Å–≤–∞—Ä–ª–∞—Ö": {
        "price": "110,000‚ÇÆ (–∞–∂–ª—ã–Ω —Ü–∞–≥–∞–∞—Ä), 132,000‚ÇÆ (–∞–∂–ª—ã–Ω –±—É—Å —Ü–∞–≥–∞–∞—Ä)",
        "desc": "–ù–∏–π–ª“Ø“Ø–ª—ç–≥—á—ç—ç—Å —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π —Å–µ—Ä–≤–µ—Ä–∏–π–Ω –¥–æ—Ç–æ–æ–¥ –∞–ª–¥–∞–∞ –∏–ª—Ä“Ø“Ø–ª—ç—Ö, –∑–∞—Å–≤–∞—Ä–ª–∞—Ö",
        "server_inside": True, "server_outside": False, "duration": "–ê–ª–¥–∞–∞–Ω—ã —Ö—ç–º–∂—ç—ç–Ω—ç—ç—Å —Ö–∞–º–∞–∞—Ä–Ω–∞, —É—Ä—å–¥—á–∏–ª–∞–Ω —Ö—É–≥–∞—Ü–∞–∞ —Ç–æ–¥–æ—Ä—Ö–æ–π–ª–æ—Ö –±–æ–ª–æ–º–∂–≥“Ø–π."
    }
}


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)
